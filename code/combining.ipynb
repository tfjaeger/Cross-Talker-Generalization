{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from phonecodes import phonecodes\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textgrid\n",
    "from scipy.spatial.distance import euclidean\n",
    "import copy\n",
    "\n",
    "import jiwer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "import soundfile as sf\n",
    "\n",
    "_ESPEAK_LIBRARY = r\"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n",
    "processor_P = AutoProcessor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model_P = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_result_path=r\"..\\data\\test.xlsx\"\n",
    "human_result = pd.read_excel(human_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_result_1a=human_result[human_result[\"Experiment\"]==\"1a\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathset(paths):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(\".wav\")]\n",
    "\n",
    "def CTC_index(processor,outind):\n",
    "    meaningful_ids = []\n",
    "    meaningful_indices = []\n",
    "    previous_id = -1  \n",
    "    blank_token_id = processor.tokenizer.pad_token_id  \n",
    "    for i, token_id in enumerate(outind[0]):  \n",
    "        if token_id != previous_id and token_id != blank_token_id:\n",
    "            meaningful_ids.append(token_id.item())  \n",
    "            meaningful_indices.append(i)  \n",
    "        previous_id = token_id\n",
    "    \n",
    "    return meaningful_indices\n",
    "\n",
    "def get_set_diphone(paths,model,processor):\n",
    "    out_dict={}\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'ʒ', \n",
    "                    'h', 'm', 'n', 'ŋ', 'l', 'ɹ', 'w', 'j', 'tʃ', 'dʒ', \n",
    "                    'i', 'ɪ', 'eɪ', 'ɛ', 'æ', 'ɑ', 'ʌ', 'ɔ', 'oʊ', 'ʊ', 'u', \n",
    "                    'ɜː', 'ə', 'aɪ', 'aʊ', 'ɔɪ']\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    #english_phoneme_dict.values()\n",
    "    for each_sentence in paths:\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        #tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "\n",
    "        '''sentence16_end_time=tg_sentence[15].maxTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.maxTime<=sentence16_end_time]\n",
    "        tg_word = [i for i in tg_word if i.maxTime<=sentence16_end_time]'''\n",
    "        \n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        #wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        for each_tg in tg_sentence:\n",
    "            start=round(each_tg.minTime*16000)\n",
    "            end=round(each_tg.maxTime*16000)\n",
    "            input=processor(wave_res[start:end],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "            input=input.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder1=model(input).logits\n",
    "            \n",
    "            selected=out_encoder1\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            \n",
    "            #outind=torch.argmax(out_encoder1,dim=-1).cpu().numpy()\n",
    "            transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "            phonemeindex = CTC_index(processor,outind)\n",
    "            out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "            for i in range(len(transcription)-1):\n",
    "                key = transcription[i] + transcription[i + 1]\n",
    "                if key not in out_dict:\n",
    "                    out_dict[key] = []\n",
    "                out_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    "            torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def get_training_paths(TrainingTalkerID,all_path):\n",
    "    path_list=[]\n",
    "    TalkerID=[]\n",
    "    for each_ID in TrainingTalkerID.split(\", \"):\n",
    "        if each_ID[:3]==\"CMN\":\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_CMN_ENG_HT1\")\n",
    "        else:\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_ENG_ENG_HT1\")\n",
    "    \n",
    "    for each_path in TalkerID:\n",
    "        for i in all_path:\n",
    "            if each_path in i:\n",
    "                path_list.append(i)\n",
    "                break\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "\n",
    "def build_exposure_set(paths, native_dict, set_list, model,processor):\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>', 'p', 'b', 't', 'd', \n",
    "                        'k', 'g', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'ʒ', \n",
    "                        'h', 'm', 'n', 'ŋ', 'l', 'ɹ', 'w', 'j', 'tʃ', 'dʒ', \n",
    "                        'i', 'ɪ', 'eɪ', 'ɛ', 'æ', 'ɑ', 'ʌ', 'ɔ', 'oʊ', 'ʊ', 'u', \n",
    "                        'ɜː', 'ə', 'aɪ', 'aʊ', 'ɔɪ']\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    english_phoneme_dict.values()\n",
    "    for each_sentence in paths:\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "        tg_sentence = [each for _,each in enumerate(tg_sentence) if _ in set_list]\n",
    "        '''sentence16_end_time=tg_sentence[15].maxTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.maxTime<=sentence16_end_time]\n",
    "        tg_word = [i for i in tg_word if i.maxTime<=sentence16_end_time]'''\n",
    "        \n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        #wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        for each_tg in tg_sentence:\n",
    "            start=round(each_tg.minTime*16000)\n",
    "            end=round(each_tg.maxTime*16000)\n",
    "            input=processor(wave_res[start:end],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "            input=input.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder1=model(input).logits\n",
    "            selected=out_encoder1\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            #outind=torch.argmax(out_encoder1,dim=-1).cpu().numpy()\n",
    "            transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "            phonemeindex = CTC_index(processor,outind)\n",
    "            out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "            for i in range(len(transcription)-1):\n",
    "                key = transcription[i] + transcription[i + 1]\n",
    "                if key not in native_dict:\n",
    "                    native_dict[key] = []\n",
    "                native_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    return native_dict\n",
    "    #'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\ALL_032_M_CMN_ENG_HT1.wav'\n",
    "def get_test_list(file_path,key_word,sentenceID,model,processor):\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'ʒ', \n",
    "                    'h', 'm', 'n', 'ŋ', 'l', 'ɹ', 'w', 'j', 'tʃ', 'dʒ', \n",
    "                    'i', 'ɪ', 'eɪ', 'ɛ', 'æ', 'ɑ', 'ʌ', 'ɔ', 'oʊ', 'ʊ', 'u', \n",
    "                    'ɜː', 'ə', 'aɪ', 'aʊ', 'ɔɪ']\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "\n",
    "    sentenceID=int(sentenceID[-3:])-1\n",
    "    #file_path= f'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\{file_path[:-5]}.wav'\n",
    "    \n",
    "    tg = textgrid.TextGrid.fromFile(file_path[:-3]+\"TextGrid\")\n",
    "    tg_sentence = [i for i in tg[0] if i.mark!=\"\"][sentenceID]\n",
    "    \n",
    "    tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "    \n",
    "    wave, sr = librosa.load(file_path)\n",
    "    wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "    \n",
    "\n",
    "    for each_word_tg in tg_word:\n",
    "        if each_word_tg.minTime >= tg_sentence.minTime and each_word_tg.maxTime <= tg_sentence.maxTime:\n",
    "            #print(each_word_tg.mark.lower(),key_word)\n",
    "            if each_word_tg.mark.lower()==key_word:\n",
    "                start=each_word_tg.minTime\n",
    "                end=each_word_tg.maxTime\n",
    "                break\n",
    "                #print(\"start:\",start,\"end:\",end)\n",
    "    #word_length=len(wave_res)/16000\n",
    "    out_list=[]\n",
    "    \n",
    "    sentence_total_length=tg_sentence.maxTime-tg_sentence.minTime\n",
    "    word_cut_start=start-tg_sentence.minTime\n",
    "    word_cut_end=end-tg_sentence.minTime\n",
    "    \n",
    "    input=processor(wave_res[int(tg_sentence.minTime*16000):round(tg_sentence.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_encoder=model(input.to(device)).logits\n",
    "        out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().numpy()\n",
    "    \n",
    "    word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "    word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "    \n",
    "    selected=out_encoder[:,word_start:word_end,:]\n",
    "    mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "    mask[list(english_phoneme_dict.values())] = False\n",
    "    selected[:, :, mask] = 0\n",
    "    outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "    phonemeindex = CTC_index(processor,outind)\n",
    "    transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "\n",
    "    \n",
    "    if len(phonemeindex)<2:\n",
    "        each_FE = out_FE[word_start:,:]\n",
    "        selected=out_encoder[:,word_start:,:]\n",
    "        mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "        mask[list(english_phoneme_dict.values())] = False\n",
    "        selected[:, :, mask] = 0\n",
    "        outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "        phonemeindex = CTC_index(processor,outind)\n",
    "        transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "        \n",
    "        diphone_key = transcription[0] + transcription[0 + 1]\n",
    "        out_list.append((diphone_key, np.vstack((each_FE[phonemeindex[0]], each_FE[phonemeindex[0 + 1]]))))\n",
    "\n",
    "    else:\n",
    "        each_FE = out_FE[word_start:word_end,:]\n",
    "        for i in range(len(transcription)-1):\n",
    "            diphone_key = transcription[i] + transcription[i + 1]\n",
    "            out_list.append((diphone_key, np.vstack((each_FE[phonemeindex[i]], each_FE[phonemeindex[i + 1]]))))\n",
    "    torch.cuda.empty_cache()\n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_ENG_ENG_path=r\"..\\data\\raw_L1\"\n",
    "ALL_ENG_ENG_pathset=get_pathset(ALL_ENG_ENG_path)\n",
    "ALL_ENG_ENG_dict = get_set_diphone(ALL_ENG_ENG_pathset, model_P, processor_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "all_path=get_pathset(r\"..\\data\\raw\")\n",
    "all_ENG_ENG_pathset=[s.replace(\"raw_L1\", \"raw\") for s in get_pathset(r\"..\\data\\raw_L1\")]\n",
    "training_talker_loc=human_result_1a.columns.get_loc(\"TrainingTalkerID\")\n",
    "TrainingTalkerID = human_result_1a.values[0][training_talker_loc]\n",
    "\n",
    "if get_training_paths(TrainingTalkerID,all_path)[0] in all_ENG_ENG_pathset:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_sentence=all_ENG_ENG_pathset[19]\n",
    "tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "tg_sentence = [each for _,each in enumerate(tg_sentence) if _ in set1_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENG_M_070, ENG_M_133, ENG_M_066, ENG_M_131, ENG_M_055'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingTalkerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\raw_L1\\\\ALL_ENG_ENG_HT1\\\\ALL_070_M_ENG_ENG_HT1.wav'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_list(test_file[0], key_word, sentenceID, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT1\\\\ALL_070_M_ENG_ENG_HT1.wav'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename_loc=df.columns.get_loc(\"Filename\")\n",
    "test_file = [each for each in all_path if os.path.split(human_result_1a.values[2423][filename_loc])[-1][:-5] in each]\n",
    "training_files_path=get_training_paths(TrainingTalkerID,all_path)[0]\n",
    "training_files_path=get_training_paths(TrainingTalkerID,all_path)\n",
    "training_dict=build_exposure_set(training_files_path, all_eng_dict,train_set, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=human_result_1a\n",
    "each_=df.values[2423]\n",
    "filename_loc=df.columns.get_loc(\"Filename\")\n",
    "keyword_loc=df.columns.get_loc(\"Keyword\")\n",
    "training_talker_loc=df.columns.get_loc(\"TrainingTalkerID\")\n",
    "\n",
    "all_path=get_pathset(r\"..\\data\\raw\")\n",
    "all_ENG_ENG_pathset=get_pathset(r\"..\\data\\raw_L1\")\n",
    "\n",
    "set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "if each_[df.columns.get_loc(\"TrainingTestSet\")] == \"set2,set1\":\n",
    "    train_set=set2_list\n",
    "    test_set=set1_list\n",
    "else:\n",
    "    train_set=set1_list\n",
    "    test_set=set2_list\n",
    "\n",
    "#print(each_[filename_loc])\n",
    "test_file = [each for each in all_path if os.path.split(each_[filename_loc])[-1][:-5] in each]\n",
    "#print(test_file)\n",
    "key_word = each_[keyword_loc] #string\n",
    "TrainingTalkerID = each_[training_talker_loc] #list of string\n",
    "sentenceID = each_[df.columns.get_loc(\"SentenceID\")]\n",
    "training_files_path=get_training_paths(TrainingTalkerID,all_path)\n",
    "training_dict=build_exposure_set(training_files_path, copy.deepcopy(ALL_ENG_ENG_dict),train_set, model_P, processor_P)\n",
    "test_list= get_test_list(test_file[0],key_word,sentenceID,model_P,processor_P)# word level, list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in [i[0] for i in test_list]:\n",
    "    print(i in training_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Big dogs can be dangerous.', '2e2d58024ab7c3cd1300ffbfa25b8e08',\n",
       "       'test', 6, 2, 'talker_CMN_M_035/ALL_035_M_CMN_ENG_HT1_S003',\n",
       "       'big dogs can be dangerous', '5Acc_Diff1Acc', 'set2,set1', 'a',\n",
       "       nan, 35, 'ALL_035_M_CMN', 5, 'at_work', 'no', nan, '5Acc_Diff1Acc',\n",
       "       nan, 'Phillipines', 'monolingual', 'I have only lived in America.',\n",
       "       'a', 41.0, 'NonHisp', 'RSRB00045955', 'white;', nan, 'Male', '5',\n",
       "       '2017-02-06T15:30:45Z', 'yes', 373, 'HT1_S003',\n",
       "       'big, dogs, be, dangerous', 4, 51, 1.0, 4,\n",
       "       'Multi-talker\\ntraining', 'Multi-talker', 'month', 'excellent',\n",
       "       'headphones', 'in-ear', 35, 'CMN_M_035', False, nan,\n",
       "       'CMN_M_016, CMN_M_043, CMN_M_037, CMN_M_021, CMN_M_032', 'no',\n",
       "       'no', 'yes', 'no', 'Test talker 5', 'dangerous',\n",
       "       'big dogs can be dangerous', 1, '1a', 'Multi-talker', 0, 1, 0,\n",
       "       'Multi-talker', -0.25, 0.5, 0.25, 0.3682950401014196,\n",
       "       0.2481983465853671], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_result_1a.values[2423]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\ALL_035_M_CMN_ENG_HT1.wav']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_loc=human_result_1a.columns.get_loc(\"Filename\")\n",
    "test_file = [each for each in all_path if os.path.split(human_result_1a.values[2423][filename_loc])[-1][:-5] in each]\n",
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_measure(df,all_eng_dict, model, processor):\n",
    "    sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count =[], [], [], [], []\n",
    "    \n",
    "    train_set_dict={}\n",
    "    test_word_dict={}\n",
    "    \n",
    "    for each_ in tqdm.tqdm(df.values):\n",
    "        filename_loc=df.columns.get_loc(\"Filename\")\n",
    "        keyword_loc=df.columns.get_loc(\"Keyword\")\n",
    "        training_talker_loc=df.columns.get_loc(\"TrainingTalkerID\")\n",
    "        \n",
    "        all_path=get_pathset(r\"..\\data\\raw\")\n",
    "        all_ENG_ENG_pathset=[s.replace(\"raw_L1\", \"raw\") for s in get_pathset(r\"..\\data\\raw_L1\")]\n",
    "        \n",
    "        set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "        set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "        if each_[df.columns.get_loc(\"TrainingTestSet\")] == \"set2,set1\":\n",
    "            train_set=set2_list\n",
    "            test_set=set1_list\n",
    "        else:\n",
    "            train_set=set1_list\n",
    "            test_set=set2_list\n",
    "        \n",
    "        #print(each_[filename_loc])\n",
    "        test_file = [each for each in all_path if os.path.split(each_[filename_loc])[-1][:-5] in each]\n",
    "        #print(test_file)\n",
    "        key_word = each_[keyword_loc] #string\n",
    "        TrainingTalkerID = each_[training_talker_loc] #list of string\n",
    "        sentenceID = each_[df.columns.get_loc(\"SentenceID\")]\n",
    "        training_files_path=get_training_paths(TrainingTalkerID,all_path)\n",
    "        \n",
    "        if training_files_path[0] in all_ENG_ENG_pathset:\n",
    "            training_dict=copy.deepcopy(all_eng_dict)\n",
    "        else:\n",
    "            if TrainingTalkerID not in train_set_dict:\n",
    "                train_set_dict[TrainingTalkerID]={}\n",
    "                \n",
    "            if each_[df.columns.get_loc(\"TrainingTestSet\")] not in train_set_dict[TrainingTalkerID]:\n",
    "                training_dict=build_exposure_set(training_files_path, copy.deepcopy(all_eng_dict), train_set, model, processor)\n",
    "                train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]=copy.deepcopy(training_dict)\n",
    "            else:\n",
    "                training_dict=train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]\n",
    "\n",
    "    \n",
    "        if test_file[0] not in test_word_dict:\n",
    "            test_word_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_word_dict[test_file[0]]:\n",
    "            test_word_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_word_dict[test_file[0]][sentenceID]:\n",
    "            test_list = get_test_list(test_file[0], key_word, sentenceID, model, processor)\n",
    "            test_word_dict[test_file[0]][sentenceID][key_word]=copy.deepcopy(test_list)\n",
    "        else:\n",
    "            test_list=test_word_dict[test_file[0]][sentenceID][key_word]\n",
    "            \n",
    "        # word level, list\n",
    "        sim_max=[]\n",
    "        sim_std=[]\n",
    "        sim_mean=[]\n",
    "        isincluded=[]\n",
    "        #sim_count=[]\n",
    "        for each_diphone in test_list:\n",
    "            \n",
    "            sims=[]\n",
    "            if each_diphone[0] in training_dict.keys():\n",
    "                isincluded.append(1)\n",
    "                \n",
    "                for each_vec in training_dict[each_diphone[0]]:\n",
    "                    d=euclidean(each_diphone[1].ravel(),each_vec.ravel())\n",
    "                    sim=np.exp(-0.1*d)\n",
    "                    sims.append(sim)\n",
    "                    #sims.append(0)\n",
    "            else:\n",
    "                isincluded.append(0)\n",
    "                sims.append(0)\n",
    "            #sim_count.append(len(sim))\n",
    "            sim_max.append(np.max(sims))\n",
    "            sim_std.append(np.std(sims))\n",
    "            sim_mean.append(np.mean(sims))\n",
    "            \n",
    "            \n",
    "        sim_mean_max=np.mean(sim_max)\n",
    "        sim_mean_std=np.mean(sim_std)\n",
    "        sim_mean_mean=np.mean(sim_mean)\n",
    "        sim_mean_max_list.append(sim_mean_max)\n",
    "        sim_mean_std_list.append(sim_mean_std)\n",
    "        sim_mean_mean_list.append(sim_mean_mean)\n",
    "        isincluded_list.append(np.count_nonzero(isincluded))\n",
    "        diphone_count.append(len(isincluded))\n",
    "        \n",
    "    return sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16477/16477 [12:00<00:00, 22.88it/s] \n"
     ]
    }
   ],
   "source": [
    "sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count=sim_measure(human_result_1a,ALL_ENG_ENG_dict,model_P, processor_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_343840\\2556233308.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_max\"]=sim_mean_max_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_343840\\2556233308.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_std\"]=sim_mean_std_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_343840\\2556233308.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_mean\"] = sim_mean_mean_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_343840\\2556233308.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"diphone_overlapped\"]=isincluded_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_343840\\2556233308.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"NumDiphone_word\"]=diphone_count\n"
     ]
    }
   ],
   "source": [
    "human_result_1a[\"sim_mean_max\"]=sim_mean_max_list\n",
    "human_result_1a[\"sim_mean_std\"]=sim_mean_std_list\n",
    "human_result_1a[\"sim_mean_mean\"] = sim_mean_mean_list\n",
    "human_result_1a[\"diphone_overlapped\"]=isincluded_list\n",
    "human_result_1a[\"NumDiphone_word\"]=diphone_count\n",
    "human_result_1a.to_excel('similarities.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
